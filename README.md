# ðŸ“§ Spam vs. Ham â€“ NLP Miniâ€‘Project

A beginnerâ€‘friendly walkthrough that turns raw SMS messages into a spam filter using
**TFâ€‘IDF features** and a **Random Forest** classifier.

![Class Balance](class_balance.png)

## QuickÂ start

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
jupyter notebook spam_rf_clean.ipynb
```

## Project structure

| File | Purpose |
|------|---------|
| `spam_rf_clean.ipynb` | Stepâ€‘byâ€‘step notebook (data prep â†’ modelling â†’ evaluation) |
| `requirements.txt`             | Minimal Python dependencies |
| `class_balance.png`            | Autoâ€‘generated plot of label distribution |

## Notebook outline

This project follows a classic NLP classification flow using TF-IDF and Random Forests. It now includes full evaluation metrics for the model's performance on the test set.

| Section | What youâ€™ll learn |
|---------|-------------------|
| 1. Load data | Pandas basics, class balance |
| 2. Preâ€‘processing | Cleaning punctuation, lowerâ€‘casing |
| 3. Vectorisation | TFâ€‘IDF (unigrams + bigrams) |
| 4. Baseline model | Random Forest training & evaluation |
| 5. Feature selection | Importance >Â 0.001  |
| 6. GridSearchCV | Light hyperâ€‘parameter tuning |
| 7. Final metrics | Accuracy, precision, recall, F1-score, confusion matrix |

## Why RandomÂ Forest?

* Easy to interpret via **feature importances**  
* Handles highâ€‘dimensional sparse inputs without heavy tuning  
* A practical first step before deeper models (Logistic Regression, SVM, BERT)

## Next steps ðŸš€

1. Compare with **LinearÂ SVM** or **LogisticRegression**.  
2. Try `class_weight="balanced"` to mitigate class imbalance.  
3. Add character nâ€‘grams to capture obfuscated spam (e.g. `v1agra`).  
4. Persist the trained pipeline with `joblib` and expose it as an API.  
